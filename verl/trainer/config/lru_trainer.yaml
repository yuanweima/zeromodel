# LRU Trainer Configuration
# Configuration for training MLA + LRU model on causal loop task

# Data Configuration
data:
  train_files: ~/data/causal_loop/train.parquet
  val_files: ~/data/causal_loop/test.parquet
  train_batch_size: 256
  val_batch_size: 64
  max_prompt_length: 512
  max_response_length: 512
  truncation_side: right

# Model Configuration
model:
  # Base model to adapt (Qwen-0.5B for fast iteration)
  base_model: Qwen/Qwen2.5-0.5B

  # MLA Configuration
  mla:
    kv_latent_dim: 256
    q_latent_dim: 512
    rope_head_dim: 64
    attention_dropout: 0.0
    attention_bias: false

  # LRU Configuration
  lru:
    enabled: true
    max_iterations: 8
    halt_threshold: 0.99
    init_halt_bias: -2.0
    use_layer_norm: true
    gradient_checkpointing: true
    # Which layers to apply LRU (null = all)
    layers: null

# Training Configuration
training:
  # Optimizer
  optimizer: adamw
  learning_rate: 1.0e-5
  weight_decay: 0.01
  adam_beta1: 0.9
  adam_beta2: 0.999
  adam_epsilon: 1.0e-8
  max_grad_norm: 1.0

  # Schedule
  warmup_steps: 100
  total_training_steps: 10000
  lr_scheduler_type: cosine

  # Precision
  bf16: true
  fp16: false

  # Gradient accumulation
  gradient_accumulation_steps: 4

  # Checkpointing
  save_steps: 500
  save_total_limit: 3
  output_dir: ./outputs/lru_causal_loop

# LRU Loss Configuration
lru_loss:
  stability_weight: 0.1
  sparsity_weight: 0.01
  ponder_weight: 0.001
  stability_decay: 0.9
  sparsity_target: 0.8
  warmup_steps: 200
  weight_schedule: linear  # constant, linear, cosine

# PPO Configuration (for RL fine-tuning)
ppo:
  # Actor-Critic
  actor_lr: 1.0e-6
  critic_lr: 1.0e-5

  # PPO hyperparameters
  ppo_epochs: 4
  clip_ratio: 0.2
  value_loss_coef: 0.5
  entropy_coef: 0.01

  # GAE
  gamma: 0.99
  gae_lambda: 0.95

  # KL control
  kl_coef: 0.1
  kl_target: 0.02

  # Rollout
  rollout_batch_size: 128
  max_new_tokens: 256
  temperature: 0.7
  top_p: 0.95

# Reward Configuration
reward:
  style: rule
  reward_module: verl.utils.reward_score.causal_loop
  # Reward scaling
  correct_reward: 1.0
  partial_reward_base: 0.3
  format_reward: 0.1

# Evaluation Configuration
evaluation:
  eval_steps: 200
  eval_batch_size: 64
  # Metrics to track
  metrics:
    - accuracy_by_level
    - avg_iterations
    - convergence_ratio
    - ponder_cost

# Distributed Training
distributed:
  # FSDP
  fsdp_enabled: true
  fsdp_sharding_strategy: FULL_SHARD
  fsdp_offload_params: false
  fsdp_offload_optimizer: false

  # Data parallel
  world_size: 1  # Set via command line

  # Sequence parallel (for long sequences)
  ulysses_sequence_parallel_size: 1

# Logging
logging:
  log_level: INFO
  log_steps: 10
  wandb:
    enabled: true
    project: zeromodel
    name: lru_causal_loop
    tags:
      - mla
      - lru
      - causal_loop

# Experiment Groups Configuration
# For ablation studies
#
# Parameter counts (for Qwen2.5-0.5B base):
#   - baseline_original:  630M params
#   - baseline_matched:   646M params (intermediate_size=5120)
#   - mla_only:           634M params
#   - mla_lru_full:       649M params (+2.2% from LRU)
#
experiments:
  # ============================================================================
  # BASELINES (for fair comparison)
  # ============================================================================

  # Original Qwen-0.5B without modifications
  baseline:
    model:
      mla:
        enabled: false
      lru:
        enabled: false
    description: "Baseline Qwen-0.5B (630M params)"

  # Parameter-matched baseline: increase FFN size to match MLA+LRU
  baseline_matched:
    model:
      intermediate_size: 5120  # Adjusted to match MLA+LRU param count (~646M)
      mla:
        enabled: false
      lru:
        enabled: false
    description: "Parameter-matched baseline (646M params, fair comparison)"

  # ============================================================================
  # MLA ABLATION
  # ============================================================================

  # MLA only (no LRU) - tests compression alone
  mla_only:
    model:
      mla:
        enabled: true
      lru:
        enabled: false
    description: "MLA attention without LRU (634M params)"

  # ============================================================================
  # LRU COMPONENT ABLATION
  # ============================================================================

  # Full MLA + LRU (fixed iterations)
  mla_lru_fixed:
    model:
      mla:
        enabled: true
      lru:
        enabled: true
        halt_threshold: 1.0  # Never halt early = fixed iterations
        use_positional_mixing: true
        use_global_halting: true
    description: "MLA + LRU with fixed 8 iterations"

  # Full MLA + LRU (adaptive halting) - main experiment
  mla_lru_adaptive:
    model:
      mla:
        enabled: true
      lru:
        enabled: true
        halt_threshold: 0.99
        use_positional_mixing: true
        use_global_halting: true
    description: "MLA + LRU with ACT adaptive halting (main)"

  # Ablation: No positional mixing
  mla_lru_no_pos_mix:
    model:
      mla:
        enabled: true
      lru:
        enabled: true
        halt_threshold: 0.99
        use_positional_mixing: false
        use_global_halting: true
    description: "MLA + LRU without positional mixing"

  # Ablation: No global halting
  mla_lru_no_global_halt:
    model:
      mla:
        enabled: true
      lru:
        enabled: true
        halt_threshold: 0.99
        use_positional_mixing: true
        use_global_halting: false
    description: "MLA + LRU without global halting"

  # Ablation: No pos_mix AND no global halt (minimal LRU)
  mla_lru_minimal:
    model:
      mla:
        enabled: true
      lru:
        enabled: true
        halt_threshold: 0.99
        use_positional_mixing: false
        use_global_halting: false
    description: "MLA + LRU minimal (no pos_mix, no global halt)"

  # ============================================================================
  # ITERATION COUNT ABLATION
  # ============================================================================

  mla_lru_iter_2:
    model:
      mla:
        enabled: true
      lru:
        enabled: true
        max_iterations: 2
        halt_threshold: 1.0  # Fixed
    description: "MLA + LRU with fixed 2 iterations"

  mla_lru_iter_4:
    model:
      mla:
        enabled: true
      lru:
        enabled: true
        max_iterations: 4
        halt_threshold: 1.0
    description: "MLA + LRU with fixed 4 iterations"

  mla_lru_iter_8:
    model:
      mla:
        enabled: true
      lru:
        enabled: true
        max_iterations: 8
        halt_threshold: 1.0
    description: "MLA + LRU with fixed 8 iterations"

  mla_lru_iter_16:
    model:
      mla:
        enabled: true
      lru:
        enabled: true
        max_iterations: 16
        halt_threshold: 1.0
    description: "MLA + LRU with fixed 16 iterations"

  # ============================================================================
  # LOSS WEIGHT ABLATION
  # ============================================================================

  # No stability loss
  mla_lru_no_stability:
    model:
      mla:
        enabled: true
      lru:
        enabled: true
        halt_threshold: 0.99
    lru_loss:
      stability_weight: 0.0
      sparsity_weight: 0.01
      ponder_weight: 0.001
    description: "MLA + LRU without stability loss"

  # High stability loss
  mla_lru_high_stability:
    model:
      mla:
        enabled: true
      lru:
        enabled: true
        halt_threshold: 0.99
    lru_loss:
      stability_weight: 0.5
      sparsity_weight: 0.01
      ponder_weight: 0.001
    description: "MLA + LRU with high stability weight (0.5)"

  # No ponder loss
  mla_lru_no_ponder:
    model:
      mla:
        enabled: true
      lru:
        enabled: true
        halt_threshold: 0.99
    lru_loss:
      stability_weight: 0.1
      sparsity_weight: 0.01
      ponder_weight: 0.0
    description: "MLA + LRU without ponder loss"

  # High ponder loss (encourage early stopping)
  mla_lru_high_ponder:
    model:
      mla:
        enabled: true
      lru:
        enabled: true
        halt_threshold: 0.99
    lru_loss:
      stability_weight: 0.1
      sparsity_weight: 0.01
      ponder_weight: 0.01
    description: "MLA + LRU with high ponder weight (0.01)"

  # No auxiliary losses (only CE loss)
  mla_lru_ce_only:
    model:
      mla:
        enabled: true
      lru:
        enabled: true
        halt_threshold: 0.99
    lru_loss:
      stability_weight: 0.0
      sparsity_weight: 0.0
      ponder_weight: 0.0
    description: "MLA + LRU with CE loss only (no LRU losses)"

# ============================================================================
# ABLATION STUDY SUMMARY
# ============================================================================
#
# Total experiments: 18
#
# 1. Baselines (2):
#    - baseline: original Qwen-0.5B
#    - baseline_matched: parameter-matched for fair comparison
#
# 2. Architecture (3):
#    - mla_only: just MLA compression
#    - mla_lru_fixed: MLA + LRU (fixed iter)
#    - mla_lru_adaptive: MLA + LRU (adaptive) [MAIN]
#
# 3. LRU Components (3):
#    - mla_lru_no_pos_mix: without positional mixing
#    - mla_lru_no_global_halt: without global halting
#    - mla_lru_minimal: without both
#
# 4. Iteration Count (4):
#    - mla_lru_iter_2/4/8/16
#
# 5. Loss Weights (5):
#    - mla_lru_no_stability/high_stability
#    - mla_lru_no_ponder/high_ponder
#    - mla_lru_ce_only
